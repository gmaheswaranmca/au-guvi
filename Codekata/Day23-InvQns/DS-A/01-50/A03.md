# DSA Questions 21-30 - Answers (Sorting Algorithms & Searching Algorithms)

## 21. What is the time complexity of quick sort? (Google, Amazon)

**Answer:** Quick sort has variable time complexity depending on the pivot selection and input characteristics:

**Time Complexity Analysis:**

1. **Best Case: O(n log n)**
   - Pivot divides array into two equal halves
   - Optimal partitioning at each level
   - Occurs when pivot is always the median

2. **Average Case: O(n log n)**
   - Pivot divides array into reasonably balanced parts
   - Most common scenario in practice
   - Expected performance with random pivot selection

3. **Worst Case: O(n²)**
   - Pivot is always the smallest or largest element
   - One partition is empty, other has n-1 elements
   - Occurs with already sorted arrays and poor pivot choice

**Space Complexity: O(log n) average, O(n) worst case**
- Space used for recursive function calls
- Average case: log n levels of recursion
- Worst case: n levels of recursion (like a linked list)

**Detailed Analysis:**

**Best/Average Case:**
```
Level 0: 1 array of size n
Level 1: 2 arrays of size n/2
Level 2: 4 arrays of size n/4
...
Level k: 2^k arrays of size n/2^k

Total levels: log₂(n)
Operations per level: O(n) for partitioning
Total: O(n log n)
```

**Worst Case:**
```
Level 0: 1 array of size n
Level 1: 1 array of size n-1
Level 2: 1 array of size n-2
...
Level k: 1 array of size n-k

Total levels: n
Operations per level: O(n), O(n-1), O(n-2), ...
Total: n + (n-1) + (n-2) + ... + 1 = n(n+1)/2 = O(n²)
```

**Factors Affecting Performance:**
- **Pivot selection strategy**: Random, median-of-three, etc.
- **Input distribution**: Sorted, reverse sorted, random
- **Implementation details**: Partitioning method, cutoff to insertion sort

**Optimizations to Avoid Worst Case:**
- **Randomized pivot**: Choose random element as pivot
- **Median-of-three**: Choose median of first, middle, last
- **Introsort**: Switch to heapsort when recursion depth exceeds threshold
- **Hybrid approach**: Use insertion sort for small subarrays

**Comparison with Other Algorithms:**
| Algorithm | Best | Average | Worst | Space |
|-----------|------|---------|-------|-------|
| Quick Sort | O(n log n) | O(n log n) | O(n²) | O(log n) |
| Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n) |
| Heap Sort | O(n log n) | O(n log n) | O(n log n) | O(1) |

---

## 22. What is the difference between stable and unstable sorting? (Microsoft, Zoho)

**Answer:** Stability in sorting refers to whether the relative order of equal elements is preserved after sorting.

**Stable Sorting:**
- **Definition**: Maintains the relative order of equal elements
- **Behavior**: If two elements are equal, the one that appeared first in the original array will appear first in the sorted array
- **Important for**: Multi-level sorting, maintaining secondary sort criteria

**Unstable Sorting:**
- **Definition**: Does not guarantee preservation of relative order of equal elements
- **Behavior**: Equal elements may appear in any order after sorting
- **Trade-off**: Often more efficient in terms of space or time

**Example:**
```
Original array: [(3,A), (1,B), (3,C), (2,D)]
(where first number is the key to sort by)

Stable sort result:   [(1,B), (2,D), (3,A), (3,C)]
Unstable sort result: [(1,B), (2,D), (3,C), (3,A)]
                                      ↑     ↑
                                   order changed
```

**Classification of Sorting Algorithms:**

**Stable Algorithms:**
- **Merge Sort**: Always stable
- **Insertion Sort**: Stable by nature
- **Bubble Sort**: Stable with proper implementation
- **Counting Sort**: Stable
- **Radix Sort**: Stable
- **Tim Sort**: Stable (used in Python)

**Unstable Algorithms:**
- **Quick Sort**: Unstable due to partitioning
- **Selection Sort**: Unstable due to swapping
- **Heap Sort**: Unstable due to heap operations
- **Shell Sort**: Unstable due to gap sorting

**Real-world Example:**
```
Student records: [(Alice, 85), (Bob, 90), (Charlie, 85), (David, 90)]
Sort by grade:

Stable result:   [(Alice, 85), (Charlie, 85), (Bob, 90), (David, 90)]
Unstable result: [(Charlie, 85), (Alice, 85), (David, 90), (Bob, 90)]
```

**Why Stability Matters:**
1. **Multi-level sorting**: Sort by grade, then by name
2. **Database operations**: Maintaining record order
3. **GUI applications**: Preserving user's previous sorting choices
4. **Time-series data**: Maintaining chronological order for equal values

**Making Unstable Algorithms Stable:**
- Add original index as secondary key
- Use stable partitioning methods
- Modify comparison function to include position information

**Performance Implications:**
- Stable algorithms may require extra space or time
- Unstable algorithms can be more efficient
- Trade-off between stability and performance

---

## 23. What is in-place sorting? (Facebook, Google)

**Answer:** In-place sorting is a sorting algorithm that transforms the input array into a sorted array using only a constant amount of extra memory space, regardless of the input size.

**Definition:**
- **Space Complexity**: O(1) auxiliary space
- **Behavior**: Sorts elements within the original array
- **Memory Usage**: Only uses a constant amount of additional memory for variables

**Key Characteristics:**
1. **Constant Extra Space**: Uses O(1) additional memory
2. **Original Array Modified**: Sorts elements in the original array
3. **No Additional Data Structures**: Doesn't create new arrays or lists
4. **Space Efficient**: Suitable for memory-constrained environments

**In-place Sorting Algorithms:**

**Truly In-place:**
- **Bubble Sort**: O(1) extra space
- **Selection Sort**: O(1) extra space
- **Insertion Sort**: O(1) extra space
- **Heap Sort**: O(1) extra space
- **Shell Sort**: O(1) extra space

**Pseudo In-place:**
- **Quick Sort**: O(log n) space for recursion stack
- **Introsort**: O(log n) space for recursion

**Not In-place:**
- **Merge Sort**: O(n) extra space for temporary arrays
- **Counting Sort**: O(k) extra space where k is range
- **Radix Sort**: O(n + k) extra space

**Example - Bubble Sort (In-place):**
```python
def bubble_sort_inplace(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]  # In-place swap
    return arr

# Only uses constant extra space for variables i, j, n
```

**Example - Merge Sort (Not In-place):**
```python
def merge_sort_not_inplace(arr):
    if len(arr) <= 1:
        return arr
    
    mid = len(arr) // 2
    left = merge_sort_not_inplace(arr[:mid])    # Creates new array
    right = merge_sort_not_inplace(arr[mid:])   # Creates new array
    
    return merge(left, right)  # Creates new array for result
```

**Advantages of In-place Sorting:**
1. **Memory Efficient**: Uses minimal extra memory
2. **Suitable for Large Datasets**: When memory is limited
3. **Better Cache Performance**: Works with original data location
4. **No Memory Allocation Overhead**: Faster execution

**Disadvantages of In-place Sorting:**
1. **May Be Slower**: Some in-place algorithms are slower
2. **Original Data Lost**: Cannot preserve original order
3. **Implementation Complexity**: May be harder to implement
4. **Limited Parallelization**: Harder to parallelize

**Space Complexity Comparison:**
| Algorithm | Space Complexity | In-place? |
|-----------|------------------|-----------|
| Bubble Sort | O(1) | Yes |
| Selection Sort | O(1) | Yes |
| Insertion Sort | O(1) | Yes |
| Merge Sort | O(n) | No |
| Quick Sort | O(log n) | Pseudo |
| Heap Sort | O(1) | Yes |
| Counting Sort | O(k) | No |

**When to Use In-place Sorting:**
- **Memory-constrained environments**
- **Large datasets that don't fit in memory**
- **Embedded systems with limited RAM**
- **When original data doesn't need to be preserved**

**Making Non-in-place Algorithms In-place:**
- Use iterative instead of recursive approach
- Implement custom merging without extra arrays
- Use bit manipulation for extra information storage

---

## 24. When would you use insertion sort over merge sort? (Amazon, Microsoft)

**Answer:** Despite merge sort having better asymptotic complexity, insertion sort is preferred in several specific scenarios:

**When to Choose Insertion Sort:**

1. **Small Arrays (n < 10-50)**
   - **Reason**: Lower overhead, simpler operations
   - **Performance**: Faster for small datasets despite O(n²) complexity
   - **Example**: Sorting small lists in GUI applications

2. **Nearly Sorted Arrays**
   - **Reason**: Insertion sort is adaptive (O(n) best case)
   - **Performance**: Merge sort always O(n log n) regardless of input
   - **Example**: Sorting data that arrives in mostly sorted order

3. **Memory-Constrained Environments**
   - **Reason**: Insertion sort uses O(1) space vs merge sort's O(n)
   - **Performance**: No additional memory allocation needed
   - **Example**: Embedded systems, mobile devices

4. **Online Algorithms**
   - **Reason**: Can sort data as it arrives
   - **Performance**: Doesn't need entire dataset at once
   - **Example**: Real-time data processing, streaming applications

5. **Simplicity and Reliability**
   - **Reason**: Easier to implement and debug
   - **Performance**: Lower chance of bugs, faster development
   - **Example**: Simple applications, educational purposes

**Detailed Comparison:**

| Aspect | Insertion Sort | Merge Sort |
|--------|---------------|------------|
| **Time Complexity** | O(n²) worst/avg, O(n) best | O(n log n) always |
| **Space Complexity** | O(1) | O(n) |
| **Stability** | Stable | Stable |
| **Adaptive** | Yes | No |
| **Online** | Yes | No |
| **Implementation** | Simple | Complex |

**Performance Breakpoints:**
```
For n = 10:  Insertion sort ≈ 25 operations vs Merge sort ≈ 33 operations
For n = 20:  Insertion sort ≈ 100 operations vs Merge sort ≈ 86 operations
For n = 50:  Insertion sort ≈ 625 operations vs Merge sort ≈ 282 operations
For n = 100: Insertion sort ≈ 2500 operations vs Merge sort ≈ 664 operations
```

**Real-world Scenarios:**

**1. Hybrid Sorting Algorithms:**
```python
def hybrid_sort(arr):
    if len(arr) <= 10:  # Use insertion sort for small arrays
        return insertion_sort(arr)
    else:
        return merge_sort(arr)
```

**2. Nearly Sorted Data:**
```python
# Data arriving in mostly sorted order
data = [1, 2, 3, 5, 4, 6, 7, 8, 9, 10]  # Only 5 and 4 are out of place
# Insertion sort will be much faster here
```

**3. Memory-Critical Applications:**
```python
# Embedded system with limited RAM
def sort_sensor_data(readings):  # readings is small array
    # Use insertion sort to avoid memory allocation
    return insertion_sort(readings)
```

**4. Online Processing:**
```python
# Processing streaming data
def process_stream():
    buffer = []
    while data_available():
        new_item = get_next_item()
        buffer.append(new_item)
        insertion_sort(buffer)  # Keep buffer sorted as data arrives
```

**Libraries Using Insertion Sort:**
- **Java**: Uses insertion sort for small subarrays in TimSort
- **C++ STL**: Uses insertion sort for small partitions in introsort
- **Python**: TimSort switches to insertion sort for small runs

**Practical Guidelines:**
- **Use insertion sort when**: n < 50, data is nearly sorted, memory is limited
- **Use merge sort when**: n > 100, worst-case performance matters, stability required
- **Consider hybrid approach**: Use insertion sort for small subarrays in merge sort

**Performance Optimization:**
- **Binary insertion sort**: Use binary search to find insertion position
- **Shell sort**: Extension of insertion sort with gap sequences
- **Tim sort**: Hybrid algorithm combining insertion sort and merge sort

---

## 25. What is heap sort and how does it work? (Zoho, Facebook)

**Answer:** Heap sort is a comparison-based sorting algorithm that uses a binary heap data structure. It works by building a max heap from the input data, then repeatedly extracting the maximum element and rebuilding the heap.

**How it works:**
1. **Build Max Heap**: Convert array into max heap structure
2. **Extract Maximum**: Move root (maximum) to end of array
3. **Heapify**: Restore heap property for remaining elements
4. **Repeat**: Continue until all elements are sorted

**Key Concepts:**

**Binary Heap Properties:**
- **Complete Binary Tree**: All levels filled except possibly the last
- **Heap Property**: Parent ≥ children (max heap) or parent ≤ children (min heap)
- **Array Representation**: For index i, left child = 2i+1, right child = 2i+2, parent = (i-1)/2

**Algorithm Steps:**
```
1. Build max heap from unsorted array
2. Swap root (maximum) with last element
3. Reduce heap size by 1
4. Heapify root to maintain heap property
5. Repeat steps 2-4 until heap size becomes 1
```

**Implementation Example:**
```python
def heap_sort(arr):
    n = len(arr)
    
    # Build max heap
    for i in range(n // 2 - 1, -1, -1):
        heapify(arr, n, i)
    
    # Extract elements from heap one by one
    for i in range(n - 1, 0, -1):
        arr[0], arr[i] = arr[i], arr[0]  # Move max to end
        heapify(arr, i, 0)  # Heapify reduced heap
    
    return arr

def heapify(arr, n, i):
    largest = i  # Initialize largest as root
    left = 2 * i + 1
    right = 2 * i + 2
    
    # If left child is larger than root
    if left < n and arr[left] > arr[largest]:
        largest = left
    
    # If right child is larger than largest so far
    if right < n and arr[right] > arr[largest]:
        largest = right
    
    # If largest is not root
    if largest != i:
        arr[i], arr[largest] = arr[largest], arr[i]
        heapify(arr, n, largest)  # Recursively heapify affected sub-tree
```

**Example Trace:**
```
Initial: [4, 10, 3, 5, 1]

Step 1: Build Max Heap
Array: [4, 10, 3, 5, 1]
Tree:     4
        /   \
       10    3
      /  \
     5    1

After heapify: [10, 5, 3, 4, 1]
Tree:     10
        /   \
       5     3
      /  \
     4    1

Step 2: Extract and Sort
Extract 10: [1, 5, 3, 4, 10]
Heapify:    [5, 4, 3, 1, 10]

Extract 5:  [1, 4, 3, 5, 10]
Heapify:    [4, 1, 3, 5, 10]

Extract 4:  [3, 1, 4, 5, 10]
Heapify:    [3, 1, 4, 5, 10]

Extract 3:  [1, 3, 4, 5, 10]
Final:      [1, 3, 4, 5, 10]
```

**Time Complexity:**
- **Building heap**: O(n)
- **Extracting n elements**: n × O(log n) = O(n log n)
- **Overall**: O(n log n) for all cases

**Space Complexity:**
- **O(1)**: In-place sorting algorithm
- **No extra arrays needed**: Only constant extra space for variables

**Characteristics:**
- **Not stable**: Equal elements may change relative order
- **In-place**: Uses constant extra space
- **Consistent**: O(n log n) for all cases
- **Not adaptive**: Performance doesn't improve for sorted data

**Advantages:**
- **Guaranteed O(n log n)**: No worst-case quadratic behavior
- **In-place**: Memory efficient
- **Simple implementation**: Straightforward logic
- **Good for real-time systems**: Predictable performance

**Disadvantages:**
- **Not stable**: Cannot preserve order of equal elements
- **Poor cache performance**: Non-sequential memory access
- **Slower than quicksort**: Higher constant factors
- **Not adaptive**: Doesn't benefit from partially sorted data

**Applications:**
- **Priority queues**: Fundamental data structure
- **Selection algorithms**: Finding kth largest element
- **Graph algorithms**: Dijkstra's shortest path
- **Memory management**: Heap allocation in operating systems

---

## 26. What is linear search and how does it work? (Google, Amazon)

**Answer:** Linear search is a simple searching algorithm that checks every element in a sequence sequentially until the desired element is found or the entire sequence has been searched.

**How it works:**
1. **Start from the beginning**: Begin with the first element
2. **Compare each element**: Check if current element matches the target
3. **Return if found**: Return the index when target is found
4. **Continue if not found**: Move to the next element
5. **Return not found**: If end is reached without finding target

**Algorithm Steps:**
```
1. Start from index 0
2. Compare arr[i] with target
3. If arr[i] == target, return i
4. If arr[i] != target, increment i
5. Repeat until i reaches array length
6. If not found, return -1
```

**Implementation Example:**
```python
def linear_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i  # Return index of found element
    return -1  # Return -1 if not found

# Alternative implementation
def linear_search_alternative(arr, target):
    for i, element in enumerate(arr):
        if element == target:
            return i
    return -1

# With early termination for multiple occurrences
def linear_search_all(arr, target):
    indices = []
    for i in range(len(arr)):
        if arr[i] == target:
            indices.append(i)
    return indices if indices else [-1]
```

**Example Trace:**
```
Array: [64, 34, 25, 12, 22, 11, 90]
Target: 22

Step 1: Check arr[0] = 64, 64 ≠ 22, continue
Step 2: Check arr[1] = 34, 34 ≠ 22, continue
Step 3: Check arr[2] = 25, 25 ≠ 22, continue
Step 4: Check arr[3] = 12, 12 ≠ 22, continue
Step 5: Check arr[4] = 22, 22 = 22, found! Return 4

Result: Element found at index 4
```

**Characteristics:**
- **Simple**: Easy to understand and implement
- **No preprocessing**: Works on unsorted data
- **Sequential**: Checks elements one by one
- **Exhaustive**: May need to check all elements

**Advantages:**
1. **Simple implementation**: Easy to code and understand
2. **Works on unsorted data**: No sorting required
3. **Small overhead**: Minimal extra operations
4. **Works with any data type**: Only requires equality comparison
5. **No preprocessing**: Can search immediately
6. **Memory efficient**: Only needs input array

**Disadvantages:**
1. **Slow for large datasets**: O(n) time complexity
2. **Inefficient**: May check every element
3. **Not scalable**: Performance degrades with size
4. **No optimization**: Cannot skip elements

**Variations:**

**1. Sentinel Linear Search:**
```python
def sentinel_linear_search(arr, target):
    n = len(arr)
    last = arr[n-1]  # Store last element
    arr[n-1] = target  # Set sentinel
    
    i = 0
    while arr[i] != target:
        i += 1
    
    arr[n-1] = last  # Restore last element
    
    if i < n-1 or arr[n-1] == target:
        return i
    return -1
```

**2. Linear Search with Early Termination:**
```python
def linear_search_optimized(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
        if arr[i] > target:  # If array is sorted
            break  # Early termination
    return -1
```

**Applications:**
- **Small datasets**: When data size is small
- **Unsorted data**: When sorting is not feasible
- **One-time searches**: When searching is infrequent
- **Simple implementations**: When simplicity is preferred
- **Linked lists**: When random access is not available

**Real-world Examples:**
- **Finding contact in phone book**: When names are not alphabetically sorted
- **Searching in database**: When no index is available
- **File searching**: Looking for files in directory
- **Menu navigation**: Finding option in unsorted menu

**Performance Comparison:**
```
For array size n = 1000:
- Best case: 1 comparison
- Average case: 500 comparisons
- Worst case: 1000 comparisons

For array size n = 1,000,000:
- Best case: 1 comparison
- Average case: 500,000 comparisons
- Worst case: 1,000,000 comparisons
```

---

## 27. What is the time complexity of linear search? (Microsoft, Zoho)

**Answer:** Linear search has different time complexities depending on when the target element is found:

**Time Complexity Analysis:**

1. **Best Case: O(1)**
   - **Scenario**: Target element is at the first position
   - **Operations**: Only 1 comparison needed
   - **Example**: Searching for first element in array

2. **Average Case: O(n)**
   - **Scenario**: Target element is at random position
   - **Operations**: On average, n/2 comparisons needed
   - **Calculation**: (1 + 2 + 3 + ... + n)/n = (n+1)/2 ≈ n/2

3. **Worst Case: O(n)**
   - **Scenario**: Target element is at last position or not in array
   - **Operations**: n comparisons needed
   - **Example**: Searching for last element or non-existent element

**Space Complexity: O(1)**
- **Constant space**: Only uses variables for index and comparison
- **No additional data structures**: Works with original array
- **In-place**: No extra memory proportional to input size

**Mathematical Analysis:**

**Best Case:**
```
Target at index 0: 1 comparison
Time complexity: O(1)
```

**Average Case:**
```
Probability of finding at position i: 1/n
Expected comparisons: Σ(i=1 to n) i × (1/n) = (1/n) × Σ(i=1 to n) i
                    = (1/n) × n(n+1)/2 = (n+1)/2
For large n: (n+1)/2 ≈ n/2 = O(n)
```

**Worst Case:**
```
Target at last position or not present: n comparisons
Time complexity: O(n)
```

**Performance Analysis:**

| Array Size | Best Case | Average Case | Worst Case |
|------------|-----------|--------------|------------|
| 10 | 1 | 5 | 10 |
| 100 | 1 | 50 | 100 |
| 1,000 | 1 | 500 | 1,000 |
| 10,000 | 1 | 5,000 | 10,000 |
| 1,000,000 | 1 | 500,000 | 1,000,000 |

**Comparison with Other Search Algorithms:**

| Algorithm | Best Case | Average Case | Worst Case | Prerequisite |
|-----------|-----------|--------------|------------|--------------|
| Linear Search | O(1) | O(n) | O(n) | None |
| Binary Search | O(1) | O(log n) | O(log n) | Sorted array |
| Hash Table | O(1) | O(1) | O(n) | Hash function |
| Interpolation Search | O(1) | O(log log n) | O(n) | Sorted, uniform distribution |

**Factors Affecting Performance:**

1. **Array Size**: Larger arrays require more comparisons
2. **Target Position**: Earlier positions found faster
3. **Target Existence**: Non-existent elements require full scan
4. **Data Distribution**: No impact on linear search performance

**Optimizations:**

**1. Early Termination (for sorted arrays):**
```python
def linear_search_early_termination(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
        if arr[i] > target:  # Only works for sorted arrays
            return -1  # Target cannot be found
    return -1
```

**2. Transposition (Move to Front):**
```python
def linear_search_transposition(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            if i > 0:
                arr[i], arr[i-1] = arr[i-1], arr[i]  # Move one position forward
                return i-1
            return i
    return -1
```

**3. Move to Front:**
```python
def linear_search_move_to_front(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            if i > 0:
                # Move to front
                temp = arr[i]
                for j in range(i, 0, -1):
                    arr[j] = arr[j-1]
                arr[0] = temp
                return 0
            return i
    return -1
```

**When Linear Search is Acceptable:**
- **Small arrays**: n < 100, overhead of advanced algorithms not justified
- **Unsorted data**: When sorting cost exceeds search benefits
- **One-time searches**: When searching is infrequent
- **Memory constraints**: When space for additional data structures is limited

**When to Avoid Linear Search:**
- **Large datasets**: n > 1000, consider sorting and using binary search
- **Frequent searches**: Multiple searches on same data
- **Performance-critical applications**: When search speed is crucial

---

## 28. What is binary search and how does it work? (Facebook, Google)

**Answer:** Binary search is an efficient searching algorithm that works on sorted arrays by repeatedly dividing the search space in half. It compares the target value with the middle element and eliminates half of the remaining elements at each step.

**How it works:**
1. **Find middle element**: Calculate middle index
2. **Compare with target**: Check if middle element equals target
3. **Eliminate half**: If target is smaller, search left half; if larger, search right half
4. **Repeat**: Continue until element is found or search space is empty

**Prerequisites:**
- **Sorted array**: Array must be sorted in ascending or descending order
- **Random access**: Ability to access any element directly (arrays, not linked lists)

**Algorithm Steps:**
```
1. Set left = 0, right = n-1
2. While left ≤ right:
   a. Calculate mid = (left + right) / 2
   b. If arr[mid] == target, return mid
   c. If arr[mid] < target, set left = mid + 1
   d. If arr[mid] > target, set right = mid - 1
3. If not found, return -1
```

**Implementation Examples:**

**Iterative Implementation:**
```python
def binary_search_iterative(arr, target):
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = (left + right) // 2
        
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1
```

**Recursive Implementation:**
```python
def binary_search_recursive(arr, target, left=0, right=None):
    if right is None:
        right = len(arr) - 1
    
    if left > right:
        return -1
    
    mid = (left + right) // 2
    
    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search_recursive(arr, target, mid + 1, right)
    else:
        return binary_search_recursive(arr, target, left, mid - 1)
```

**Example Trace:**
```
Array: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
Target: 7

Iteration 1:
left=0, right=9, mid=4
arr[4] = 9, 9 > 7, so right = mid-1 = 3

Iteration 2:
left=0, right=3, mid=1
arr[1] = 3, 3 < 7, so left = mid+1 = 2

Iteration 3:
left=2, right=3, mid=2
arr[2] = 5, 5 < 7, so left = mid+1 = 3

Iteration 4:
left=3, right=3, mid=3
arr[3] = 7, 7 == 7, found! Return 3

Result: Element found at index 3
```

**Handling Edge Cases:**

**1. Overflow Prevention:**
```python
# Instead of: mid = (left + right) // 2
mid = left + (right - left) // 2  # Prevents integer overflow
```

**2. Finding First/Last Occurrence:**
```python
def binary_search_first_occurrence(arr, target):
    left, right = 0, len(arr) - 1
    result = -1
    
    while left <= right:
        mid = left + (right - left) // 2
        
        if arr[mid] == target:
            result = mid
            right = mid - 1  # Continue searching left
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return result
```

**3. Insert Position:**
```python
def binary_search_insert_position(arr, target):
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = left + (right - left) // 2
        
        if arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return left  # Insert position
```

**Binary Search Variations:**

**1. Search in Rotated Sorted Array:**
```python
def search_rotated_array(arr, target):
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = left + (right - left) // 2
        
        if arr[mid] == target:
            return mid
        
        # Left half is sorted
        if arr[left] <= arr[mid]:
            if arr[left] <= target < arr[mid]:
                right = mid - 1
            else:
                left = mid + 1
        # Right half is sorted
        else:
            if arr[mid] < target <= arr[right]:
                left = mid + 1
            else:
                right = mid - 1
    
    return -1
```

**2. Search in 2D Matrix:**
```python
def search_2d_matrix(matrix, target):
    if not matrix or not matrix[0]:
        return False
    
    m, n = len(matrix), len(matrix[0])
    left, right = 0, m * n - 1
    
    while left <= right:
        mid = left + (right - left) // 2
        mid_value = matrix[mid // n][mid % n]
        
        if mid_value == target:
            return True
        elif mid_value < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return False
```

**Applications:**
- **Database indexing**: Finding records efficiently
- **Dictionary lookups**: Word searches in sorted dictionaries
- **Library systems**: Finding books by ISBN
- **Game development**: Finding game objects by ID
- **Compiler design**: Symbol table lookups

**Advantages:**
- **Efficient**: O(log n) time complexity
- **Predictable**: Consistent performance
- **Simple**: Easy to understand and implement
- **Optimal**: Cannot do better for comparison-based search

**Disadvantages:**
- **Requires sorted data**: Preprocessing overhead
- **Random access only**: Doesn't work with linked lists
- **Not suitable for dynamic data**: Insertions/deletions break sorting

---

## 29. What is the time complexity of binary search? (Amazon, Microsoft)

**Answer:** Binary search has excellent time complexity due to its divide-and-conquer approach:

**Time Complexity Analysis:**

1. **Best Case: O(1)**
   - **Scenario**: Target element is at the middle position
   - **Operations**: Only 1 comparison needed
   - **Example**: Searching for middle element in array

2. **Average Case: O(log n)**
   - **Scenario**: Target element is at any position
   - **Operations**: log₂(n) comparisons on average
   - **Calculation**: Each comparison eliminates half of remaining elements

3. **Worst Case: O(log n)**
   - **Scenario**: Target element is at end of search path or not in array
   - **Operations**: log₂(n) comparisons maximum
   - **Example**: Searching for element requiring maximum divisions

**Space Complexity:**
- **Iterative**: O(1) - constant space
- **Recursive**: O(log n) - for recursive call stack

**Mathematical Analysis:**

**How log n is derived:**
```
Initial array size: n
After 1 comparison: n/2
After 2 comparisons: n/4
After 3 comparisons: n/8
...
After k comparisons: n/2^k

Search terminates when: n/2^k = 1
Solving: 2^k = n
Taking log: k = log₂(n)

Therefore: Time complexity = O(log n)
```

**Number of Comparisons:**
| Array Size (n) | Maximum Comparisons | log₂(n) |
|----------------|--------------------|---------| 
| 8 | 3 | 3 |
| 16 | 4 | 4 |
| 32 | 5 | 5 |
| 64 | 6 | 6 |
| 128 | 7 | 7 |
| 1,024 | 10 | 10 |
| 1,048,576 | 20 | 20 |
| 1,073,741,824 | 30 | 30 |

**Comparison with Linear Search:**

| Array Size | Linear Search (Worst) | Binary Search (Worst) | Speed Improvement |
|------------|----------------------|----------------------|-------------------|
| 100 | 100 | 7 | 14x faster |
| 1,000 | 1,000 | 10 | 100x faster |
| 10,000 | 10,000 | 14 | 714x faster |
| 100,000 | 100,000 | 17 | 5,882x faster |
| 1,000,000 | 1,000,000 | 20 | 50,000x faster |

**Detailed Complexity Analysis:**

**Best Case Example:**
```
Array: [1, 3, 5, 7, 9, 11, 13]
Target: 7 (middle element)

Step 1: mid = 3, arr[3] = 7, found!
Comparisons: 1 = O(1)
```

**Worst Case Example:**
```
Array: [1, 3, 5, 7, 9, 11, 13]
Target: 1 (requires maximum divisions)

Step 1: mid = 3, arr[3] = 7 > 1, search left half [1, 3, 5]
Step 2: mid = 1, arr[1] = 3 > 1, search left half [1]
Step 3: mid = 0, arr[0] = 1 = 1, found!
Comparisons: 3 = log₂(7) ≈ 2.8 ≈ 3
```

**Recurrence Relation:**
```
T(n) = T(n/2) + O(1)

Solving using Master Theorem:
T(n) = O(log n)
```

**Space Complexity Analysis:**

**Iterative Implementation:**
```python
def binary_search_iterative(arr, target):
    left, right = 0, len(arr) - 1  # O(1) space
    
    while left <= right:           # O(1) space
        mid = (left + right) // 2  # O(1) space
        # ... rest of algorithm
    
    return -1
# Total space: O(1)
```

**Recursive Implementation:**
```python
def binary_search_recursive(arr, target, left, right):
    if left > right:
        return -1
    
    mid = (left + right) // 2
    # ... comparisons ...
    
    return binary_search_recursive(arr, target, left, mid-1)  # O(log n) stack depth
# Total space: O(log n)
```

**Performance Factors:**

1. **Cache Performance**: Good locality of reference
2. **Branch Prediction**: Predictable branching patterns
3. **Memory Access**: Random access requirement
4. **Preprocessing Cost**: Sorting overhead if unsorted

**Optimizations:**

**1. Interpolation Search** (for uniformly distributed data):
```python
# Average case: O(log log n)
# Worst case: O(n)
def interpolation_search(arr, target):
    left, right = 0, len(arr) - 1
    
    while left <= right and arr[left] <= target <= arr[right]:
        # Interpolation formula
        pos = left + ((target - arr[left]) * (right - left)) // (arr[right] - arr[left])
        
        if arr[pos] == target:
            return pos
        elif arr[pos] < target:
            left = pos + 1
        else:
            right = pos - 1
    
    return -1
```

**2. Exponential Search** (for unbounded arrays):
```python
# Time: O(log n), where n is position of target
def exponential_search(arr, target):
    if arr[0] == target:
        return 0
    
    # Find range for binary search
    i = 1
    while i < len(arr) and arr[i] <= target:
        i *= 2
    
    # Binary search in found range
    return binary_search(arr, target, i // 2, min(i, len(arr) - 1))
```

**When Binary Search is Optimal:**
- **Large sorted datasets**: Significant performance improvement
- **Frequent searches**: Multiple searches on same data
- **Memory is abundant**: Can afford to keep data sorted
- **Static data**: Data doesn't change frequently

**Limitations:**
- **Requires sorted data**: O(n log n) preprocessing cost
- **Random access needed**: Doesn't work with linked lists
- **Not suitable for dynamic data**: Insertions/deletions expensive

---

## 30. What is the prerequisite for binary search? (Zoho, Facebook)

**Answer:** Binary search has specific prerequisites that must be met for the algorithm to work correctly:

**Primary Prerequisites:**

1. **Sorted Array**
   - **Requirement**: Array must be sorted in ascending or descending order
   - **Reason**: Binary search depends on order to eliminate half of elements
   - **Example**: [1, 3, 5, 7, 9] (ascending) or [9, 7, 5, 3, 1] (descending)

2. **Random Access**
   - **Requirement**: Direct access to any element by index
   - **Reason**: Need to quickly access middle element
   - **Supported**: Arrays, vectors, dynamic arrays
   - **Not supported**: Linked lists, trees (without indexing)

**Detailed Analysis:**

**1. Why Sorted Array is Essential:**
```python
# Correct: Sorted array
arr = [1, 3, 5, 7, 9, 11, 13]
target = 7

# Works because we can decide which half to eliminate
# If arr[mid] = 5 < 7, we know 7 must be in right half

# Incorrect: Unsorted array
arr = [7, 1, 9, 3, 5, 11, 13]
target = 7

# Doesn't work because arr[mid] = 3 < 7
# But 7 is actually in the left half!
```

**2. Why Random Access is Required:**
```python
# Arrays - Good for binary search
arr = [1, 3, 5, 7, 9]
mid = len(arr) // 2
middle_element = arr[mid]  # O(1) access

# Linked lists - Bad for binary search
class ListNode:
    def __init__(self, val=0, next=None):
        self.val = val
        self.next = next

# To find middle element in linked list: O(n) time
def find_middle(head):
    current = head
    for i in range(n // 2):  # Need to traverse
        current = current.next
    return current.val
```

**Additional Prerequisites:**

**3. Comparison Operations**
   - **Requirement**: Elements must be comparable
   - **Operations**: <, >, ==, <=, >= must be defined
   - **Example**: Numbers, strings, custom objects with comparison methods

**4. Consistency**
   - **Requirement**: Comparison results must be consistent
   - **Problem**: Floating-point precision issues
   - **Solution**: Use appropriate epsilon for floating-point comparisons

**5. No Duplicate Handling Specification**
   - **Consideration**: How to handle duplicate elements
   - **Options**: Find any, first, or last occurrence
   - **Implementation**: Different for each requirement

**Implementation Examples:**

**1. Basic Binary Search (Sorted Array):**
```python
def binary_search_basic(arr, target):
    # Prerequisite: arr must be sorted
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1
```

**2. Binary Search with Custom Comparator:**
```python
def binary_search_custom(arr, target, compare_func):
    # Prerequisite: arr must be sorted according to compare_func
    left, right = 0, len(arr) - 1
    
    while left <= right:
        mid = (left + right) // 2
        comparison = compare_func(arr[mid], target)
        
        if comparison == 0:    # Equal
            return mid
        elif comparison < 0:   # arr[mid] < target
            left = mid + 1
        else:                  # arr[mid] > target
            right = mid - 1
    
    return -1

# Example usage
def compare_strings(a, b):
    if a < b: return -1
    elif a > b: return 1
    else: return 0

names = ["Alice", "Bob", "Charlie", "David"]
index = binary_search_custom(names, "Charlie", compare_strings)
```

**3. Binary Search for Objects:**
```python
class Student:
    def __init__(self, name, grade):
        self.name = name
        self.grade = grade
    
    def __lt__(self, other):
        return self.grade < other.grade
    
    def __eq__(self, other):
        return self.grade == other.grade

def binary_search_students(students, target_grade):
    # Prerequisite: students must be sorted by grade
    left, right = 0, len(students) - 1
    
    while left <= right:
        mid = (left + right) // 2
        if students[mid].grade == target_grade:
            return mid
        elif students[mid].grade < target_grade:
            left = mid + 1
        else:
            right = mid - 1
    
    return -1
```

**Common Violations and Solutions:**

**1. Unsorted Array:**
```python
# Problem
arr = [3, 1, 4, 1, 5, 9, 2, 6]
result = binary_search(arr, 5)  # May not work correctly

# Solution
arr.sort()  # O(n log n)
result = binary_search(arr, 5)  # Now works correctly
```

**2. Linked List:**
```python
# Problem: Cannot use binary search directly on linked list
# Solution: Convert to array or use different search method

def linked_list_to_array(head):
    arr = []
    current = head
    while current:
        arr.append(current.val)
        current = current.next
    return arr

# Or use two-pointer technique for finding middle
def find_middle_slow_fast(head):
    slow = fast = head
    while fast and fast.next:
        slow = slow.next
        fast = fast.next.next
    return slow
```

**3. Duplicate Elements:**
```python
# Problem: Which duplicate to return?
arr = [1, 2, 2, 2, 3, 4, 5]
# Binary search may return any of the 2's

# Solution: Specify behavior
def binary_search_first(arr, target):
    # Find first occurrence
    left, right = 0, len(arr) - 1
    result = -1
    
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            result = mid
            right = mid - 1  # Continue searching left
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    
    return result
```

**Checking Prerequisites:**
```python
def can_use_binary_search(arr):
    # Check if array is sorted
    for i in range(1, len(arr)):
        if arr[i] < arr[i-1]:
            return False, "Array is not sorted"
    
    # Check if elements are comparable
    try:
        if len(arr) >= 2:
            _ = arr[0] < arr[1]
    except TypeError:
        return False, "Elements are not comparable"
    
    return True, "Ready for binary search"

# Usage
arr = [1, 3, 5, 7, 9]
can_search, message = can_use_binary_search(arr)
if can_search:
    result = binary_search(arr, 5)
else:
    print(f"Cannot use binary search: {message}")
```

**Performance Impact of Prerequisites:**
- **Sorting cost**: O(n log n) if array is unsorted
- **Random access**: O(1) for arrays, O(n) for linked lists
- **Maintenance**: Keeping array sorted during insertions/deletions

**When Prerequisites Are Not Met:**
- **Use linear search**: O(n) but works on any data
- **Use hash tables**: O(1) average case for exact matches
- **Use trees**: Binary search trees for dynamic data
- **Use specialized algorithms**: Interpolation search, exponential search

---

*Note: These answers cover the continuation of sorting algorithms and introduction to searching algorithms (Questions 21-30) from DSAQns01.md. Each answer provides comprehensive explanations with examples, implementations, and analysis suitable for technical interviews.*