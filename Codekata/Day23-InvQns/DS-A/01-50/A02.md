# DSA Questions 11-20 - Answers (Sorting Algorithms)

## 11. What is sorting and why is it important? (Google, Amazon)

**Answer:** Sorting is the process of arranging data elements in a particular order (ascending or descending) according to some comparison criterion. It's one of the most fundamental operations in computer science.

**Definition:**
- Rearranging elements of a collection so that they are in increasing or decreasing order
- Can be applied to numbers, strings, objects, or any comparable data type

**Why Sorting is Important:**

1. **Improved Search Performance**: Sorted data enables binary search (O(log n) vs O(n))
2. **Data Organization**: Makes data more readable and manageable
3. **Algorithm Optimization**: Many algorithms work faster on sorted data
4. **Database Operations**: Critical for database indexing and query optimization
5. **Data Analysis**: Essential for statistical analysis and data visualization
6. **User Experience**: Sorted results are easier to understand and navigate

**Real-world Applications:**
- Search engines ranking results
- E-commerce product listings
- Contact lists in phones
- File systems organizing files
- Social media feeds (chronological/relevance sorting)

**Types of Sorting:**
- **Internal Sorting**: Data fits in main memory
- **External Sorting**: Data doesn't fit in main memory
- **Stable Sorting**: Maintains relative order of equal elements
- **In-place Sorting**: Uses minimal extra memory

---

## 12. What is bubble sort and how does it work? (Microsoft, Zoho)

**Answer:** Bubble sort is a simple comparison-based sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.

**How it works:**
1. Compare adjacent elements from left to right
2. If the left element is greater than the right element, swap them
3. Continue this process for the entire array
4. After each pass, the largest element "bubbles up" to its correct position
5. Repeat until no swaps are needed

**Algorithm Steps:**
```
1. Start with the first element
2. Compare current element with next element
3. If current > next, swap them
4. Move to next pair
5. Repeat until end of array
6. If any swaps occurred, repeat the entire process
7. Stop when no swaps are needed
```

**Implementation Example:**
```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        swapped = False
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swapped = True
        if not swapped:  # Optimization: if no swaps, array is sorted
            break
    return arr
```

**Example Trace:**
```
Initial: [64, 34, 25, 12, 22, 11, 90]
Pass 1:  [34, 25, 12, 22, 11, 64, 90]  (90 bubbles up)
Pass 2:  [25, 12, 22, 11, 34, 64, 90]  (64 bubbles up)
Pass 3:  [12, 22, 11, 25, 34, 64, 90]  (34 bubbles up)
Pass 4:  [12, 11, 22, 25, 34, 64, 90]  (25 bubbles up)
Pass 5:  [11, 12, 22, 25, 34, 64, 90]  (22 bubbles up)
Pass 6:  [11, 12, 22, 25, 34, 64, 90]  (No swaps needed)
```

---

## 13. What is the time complexity of bubble sort? (Facebook, Google)

**Answer:** The time complexity of bubble sort varies based on the input characteristics:

**Time Complexity Analysis:**

1. **Best Case: O(n)**
   - Occurs when the array is already sorted
   - Only one pass needed to confirm no swaps required
   - With optimization flag, algorithm terminates early

2. **Average Case: O(n²)**
   - Elements are in random order
   - Requires approximately n²/2 comparisons
   - Most common scenario in practice

3. **Worst Case: O(n²)**
   - Array is sorted in reverse order
   - Maximum number of swaps needed
   - Every element needs to bubble through entire array

**Space Complexity: O(1)**
- In-place sorting algorithm
- Only uses a constant amount of extra memory
- No additional arrays or data structures needed

**Detailed Analysis:**
```
Outer loop runs: n times
Inner loop runs: (n-1) + (n-2) + ... + 1 = n(n-1)/2 times
Total comparisons: n(n-1)/2 = O(n²)
```

**Comparison with other algorithms:**
- **Bubble Sort**: O(n²) - Poor performance
- **Selection Sort**: O(n²) - Similar performance
- **Insertion Sort**: O(n²) - Better for small/nearly sorted arrays
- **Merge Sort**: O(n log n) - Much better performance
- **Quick Sort**: O(n log n) average - Much better performance

**Why Bubble Sort is Inefficient:**
- High number of comparisons and swaps
- Doesn't utilize any information about data structure
- Performance degrades quickly with larger datasets

---

## 14. What is selection sort and how does it work? (Amazon, Microsoft)

**Answer:** Selection sort is a simple comparison-based sorting algorithm that divides the input list into two parts: a sorted portion and an unsorted portion. It repeatedly finds the minimum element from the unsorted portion and moves it to the beginning of the sorted portion.

**How it works:**
1. Find the minimum element in the unsorted portion
2. Swap it with the first element of the unsorted portion
3. Move the boundary between sorted and unsorted portions one position right
4. Repeat until the entire array is sorted

**Algorithm Steps:**
```
1. Start with the entire array as unsorted
2. Find the minimum element in the unsorted portion
3. Swap it with the first element of unsorted portion
4. Mark this element as sorted
5. Repeat for remaining unsorted elements
```

**Implementation Example:**
```python
def selection_sort(arr):
    n = len(arr)
    for i in range(n):
        # Find minimum element in remaining unsorted array
        min_idx = i
        for j in range(i + 1, n):
            if arr[j] < arr[min_idx]:
                min_idx = j
        
        # Swap the found minimum element with first element
        arr[i], arr[min_idx] = arr[min_idx], arr[i]
    
    return arr
```

**Example Trace:**
```
Initial: [64, 25, 12, 22, 11]
         [sorted | unsorted]

Step 1:  [11, 25, 12, 22, 64]  (11 is minimum, swap with 64)
         [11 | 25, 12, 22, 64]

Step 2:  [11, 12, 25, 22, 64]  (12 is minimum, swap with 25)
         [11, 12 | 25, 22, 64]

Step 3:  [11, 12, 22, 25, 64]  (22 is minimum, swap with 25)
         [11, 12, 22 | 25, 64]

Step 4:  [11, 12, 22, 25, 64]  (25 is minimum, no swap needed)
         [11, 12, 22, 25 | 64]

Final:   [11, 12, 22, 25, 64]  (All elements sorted)
```

**Key Characteristics:**
- **In-place**: Uses only O(1) extra memory
- **Unstable**: Doesn't preserve relative order of equal elements
- **Not adaptive**: Performance doesn't improve for partially sorted arrays
- **Minimum swaps**: Makes at most n-1 swaps

---

## 15. What is the time complexity of selection sort? (Zoho, Facebook)

**Answer:** Selection sort has a consistent time complexity regardless of input characteristics:

**Time Complexity Analysis:**

1. **Best Case: O(n²)**
   - Even if array is already sorted, algorithm still searches for minimum
   - No early termination possible
   - Same number of comparisons as worst case

2. **Average Case: O(n²)**
   - For randomly ordered elements
   - Requires same number of comparisons as other cases
   - No variation in performance based on input order

3. **Worst Case: O(n²)**
   - Array sorted in reverse order
   - Still requires same number of comparisons
   - Performance doesn't get worse than average case

**Space Complexity: O(1)**
- In-place sorting algorithm
- Only uses constant extra space for variables
- No additional arrays needed

**Detailed Analysis:**
```
Outer loop: runs n times
Inner loop: runs (n-1) + (n-2) + ... + 1 = n(n-1)/2 times
Total comparisons: n(n-1)/2 = O(n²)
Total swaps: At most n-1 swaps = O(n)
```

**Number of Operations for different input sizes:**
- n = 10: ~45 comparisons
- n = 100: ~4,950 comparisons
- n = 1,000: ~499,500 comparisons
- n = 10,000: ~49,995,000 comparisons

**Comparison with Bubble Sort:**
| Aspect | Selection Sort | Bubble Sort |
|--------|---------------|-------------|
| Time Complexity | O(n²) always | O(n²) worst/avg, O(n) best |
| Swaps | O(n) | O(n²) |
| Stability | Unstable | Stable |
| Adaptivity | No | Yes (with optimization) |

**Why Selection Sort is Inefficient:**
- Always performs maximum comparisons
- No early termination
- Doesn't benefit from partially sorted data
- Quadratic growth makes it unsuitable for large datasets

**When to use Selection Sort:**
- Small datasets (n < 20)
- When memory is extremely limited
- When minimizing number of swaps is important
- Educational purposes to understand sorting concepts

---

## 16. What is insertion sort and how does it work? (Google, Amazon)

**Answer:** Insertion sort is a simple sorting algorithm that builds the final sorted array one element at a time. It works by taking elements from the unsorted portion and inserting them into their correct position in the sorted portion.

**How it works:**
1. Start with the second element (assume first element is sorted)
2. Take the current element and compare it with elements in the sorted portion
3. Shift larger elements to the right to make space
4. Insert the current element in its correct position
5. Repeat for all remaining elements

**Algorithm Steps:**
```
1. Start from the second element (index 1)
2. Store current element as key
3. Compare key with elements in sorted portion (left side)
4. Shift elements greater than key to the right
5. Insert key at correct position
6. Move to next element and repeat
```

**Implementation Example:**
```python
def insertion_sort(arr):
    for i in range(1, len(arr)):
        key = arr[i]  # Current element to be inserted
        j = i - 1     # Index of last element in sorted portion
        
        # Move elements greater than key one position ahead
        while j >= 0 and arr[j] > key:
            arr[j + 1] = arr[j]
            j -= 1
        
        # Insert key at correct position
        arr[j + 1] = key
    
    return arr
```

**Example Trace:**
```
Initial: [5, 2, 4, 6, 1, 3]
         [sorted | unsorted]

Step 1:  [2, 5, 4, 6, 1, 3]  (Insert 2 before 5)
         [2, 5 | 4, 6, 1, 3]

Step 2:  [2, 4, 5, 6, 1, 3]  (Insert 4 between 2 and 5)
         [2, 4, 5 | 6, 1, 3]

Step 3:  [2, 4, 5, 6, 1, 3]  (6 is already in correct position)
         [2, 4, 5, 6 | 1, 3]

Step 4:  [1, 2, 4, 5, 6, 3]  (Insert 1 at beginning)
         [1, 2, 4, 5, 6 | 3]

Step 5:  [1, 2, 3, 4, 5, 6]  (Insert 3 between 2 and 4)
         [1, 2, 3, 4, 5, 6]
```

**Key Characteristics:**
- **In-place**: Uses O(1) extra memory
- **Stable**: Maintains relative order of equal elements
- **Adaptive**: Performs well on nearly sorted arrays
- **Online**: Can sort arrays as elements are received

**Real-world Analogy:**
Like sorting playing cards in your hand - you pick up cards one by one and insert each into its correct position among the already sorted cards.

---

## 17. What is the time complexity of insertion sort? (Microsoft, Zoho)

**Answer:** Insertion sort has variable time complexity depending on the input characteristics:

**Time Complexity Analysis:**

1. **Best Case: O(n)**
   - Array is already sorted
   - Each element only needs one comparison
   - No shifting required
   - Linear time performance

2. **Average Case: O(n²)**
   - Elements are in random order
   - On average, each element needs n/2 comparisons
   - Requires shifting elements on average

3. **Worst Case: O(n²)**
   - Array is sorted in reverse order
   - Each element needs maximum comparisons and shifts
   - Every element must be moved to the beginning

**Space Complexity: O(1)**
- In-place sorting algorithm
- Only uses constant extra space
- No additional arrays required

**Detailed Analysis:**
```
Best Case:
- Each element: 1 comparison
- Total: n comparisons = O(n)

Average Case:
- Each element: n/2 comparisons on average
- Total: n × n/2 = n²/2 = O(n²)

Worst Case:
- Element i requires i comparisons
- Total: 1 + 2 + 3 + ... + n = n(n+1)/2 = O(n²)
```

**Performance Comparison:**
| Input Size | Best Case | Average Case | Worst Case |
|------------|-----------|--------------|------------|
| n = 10 | 10 ops | ~25 ops | 55 ops |
| n = 100 | 100 ops | ~2,500 ops | 5,050 ops |
| n = 1,000 | 1,000 ops | ~250,000 ops | 500,500 ops |

**Comparison with Other Sorting Algorithms:**
| Algorithm | Best | Average | Worst | Stable | In-place |
|-----------|------|---------|-------|--------|----------|
| Insertion Sort | O(n) | O(n²) | O(n²) | Yes | Yes |
| Bubble Sort | O(n) | O(n²) | O(n²) | Yes | Yes |
| Selection Sort | O(n²) | O(n²) | O(n²) | No | Yes |
| Merge Sort | O(n log n) | O(n log n) | O(n log n) | Yes | No |
| Quick Sort | O(n log n) | O(n log n) | O(n²) | No | Yes |

**Why Insertion Sort is Efficient for Small Arrays:**
- Low overhead compared to complex algorithms
- Good cache performance (local memory access)
- Adaptive nature benefits partially sorted data
- Simple implementation with fewer bugs

**When to use Insertion Sort:**
- Small datasets (n < 50)
- Nearly sorted arrays
- As a subroutine in hybrid algorithms (like Introsort)
- Online algorithms where data arrives sequentially

---

## 18. What is merge sort and how does it work? (Facebook, Google)

**Answer:** Merge sort is a divide-and-conquer algorithm that divides the array into two halves, recursively sorts both halves, and then merges the sorted halves to produce the final sorted array.

**How it works:**
1. **Divide**: Split the array into two halves
2. **Conquer**: Recursively sort both halves
3. **Combine**: Merge the two sorted halves into one sorted array

**Algorithm Steps:**
```
1. If array has 1 or 0 elements, return (base case)
2. Divide array into two halves
3. Recursively sort left half
4. Recursively sort right half
5. Merge the two sorted halves
```

**Implementation Example:**
```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    
    # Divide
    mid = len(arr) // 2
    left = arr[:mid]
    right = arr[mid:]
    
    # Conquer
    left = merge_sort(left)
    right = merge_sort(right)
    
    # Combine
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0
    
    # Compare elements and merge
    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    
    # Add remaining elements
    result.extend(left[i:])
    result.extend(right[j:])
    
    return result
```

**Example Trace:**
```
Initial: [38, 27, 43, 3, 9, 82, 10]

Divide Phase:
[38, 27, 43, 3, 9, 82, 10]
    /            \
[38, 27, 43]    [3, 9, 82, 10]
   /    \         /        \
[38]  [27, 43]  [3, 9]   [82, 10]
        /  \     /  \      /   \
      [27] [43] [3] [9]  [82] [10]

Conquer Phase (Merge):
[27, 43]    [3, 9]    [10, 82]
    \         /         /
   [27, 43]  [3, 9]   [10, 82]
        \      |      /
       [3, 9, 27, 43]  [10, 82]
              \         /
          [3, 9, 10, 27, 43, 82]

Final: [3, 9, 10, 27, 43, 82]
```

**Key Characteristics:**
- **Stable**: Maintains relative order of equal elements
- **Not in-place**: Requires O(n) extra space
- **Consistent**: Always O(n log n) time complexity
- **Parallelizable**: Can be easily parallelized

**Advantages:**
- Predictable performance
- Stable sorting
- Good for large datasets
- Excellent for external sorting

**Disadvantages:**
- Requires extra memory
- Slower than quicksort for small arrays
- More complex implementation

---

## 19. What is the time complexity of merge sort? (Amazon, Microsoft)

**Answer:** Merge sort has consistent time complexity across all cases due to its divide-and-conquer approach:

**Time Complexity Analysis:**

1. **Best Case: O(n log n)**
   - Array is already sorted
   - Still needs to divide and merge
   - Cannot skip any recursive calls

2. **Average Case: O(n log n)**
   - Elements are in random order
   - Same divide-and-conquer process
   - Consistent performance

3. **Worst Case: O(n log n)**
   - Array is sorted in reverse order
   - Still requires same number of operations
   - No performance degradation

**Space Complexity: O(n)**
- Requires additional arrays for merging
- Each recursive call uses O(n) space in total
- Not an in-place sorting algorithm

**Detailed Analysis:**

**Divide Phase:**
```
Level 0: 1 array of size n
Level 1: 2 arrays of size n/2
Level 2: 4 arrays of size n/4
...
Level k: 2^k arrays of size n/2^k

Total levels: log₂(n) levels
```

**Merge Phase:**
```
At each level: O(n) operations to merge all subarrays
Total levels: log₂(n)
Total operations: O(n) × log₂(n) = O(n log n)
```

**Recurrence Relation:**
```
T(n) = 2T(n/2) + O(n)
     = 2[2T(n/4) + O(n/2)] + O(n)
     = 4T(n/4) + O(n) + O(n)
     = 4T(n/4) + 2O(n)
     ...
     = nT(1) + O(n) × log₂(n)
     = O(n log n)
```

**Performance Comparison:**
| Input Size | Operations | Comparison with O(n²) |
|------------|------------|----------------------|
| n = 10 | ~33 | vs 100 (3x faster) |
| n = 100 | ~664 | vs 10,000 (15x faster) |
| n = 1,000 | ~9,966 | vs 1,000,000 (100x faster) |
| n = 10,000 | ~132,877 | vs 100,000,000 (750x faster) |

**Why O(n log n) is Optimal:**
- Comparison-based sorting has lower bound of O(n log n)
- Merge sort achieves this optimal complexity
- Cannot do better than O(n log n) for general comparison sorting

**Memory Usage:**
- Each level uses O(n) total space
- Maximum depth is log₂(n)
- Total space complexity: O(n)

**Practical Implications:**
- Excellent for large datasets
- Predictable performance for real-time systems
- Preferred for external sorting (data doesn't fit in memory)
- Good choice when stability is required

---

## 20. What is quick sort and how does it work? (Zoho, Facebook)

**Answer:** Quick sort is a highly efficient divide-and-conquer sorting algorithm that works by selecting a 'pivot' element and partitioning the array around it, such that elements smaller than the pivot come before it and elements greater come after it.

**How it works:**
1. **Choose a pivot**: Select an element from the array
2. **Partition**: Rearrange array so elements < pivot are on left, elements > pivot are on right
3. **Recursively sort**: Apply quicksort to the left and right subarrays
4. **Combine**: No explicit combining needed as sorting is done in-place

**Algorithm Steps:**
```
1. If array has 1 or 0 elements, return (base case)
2. Choose pivot element (first, last, middle, or random)
3. Partition array around pivot
4. Recursively sort left subarray (elements < pivot)
5. Recursively sort right subarray (elements > pivot)
```

**Implementation Example:**
```python
def quick_sort(arr, low=0, high=None):
    if high is None:
        high = len(arr) - 1
    
    if low < high:
        # Partition and get pivot index
        pivot_index = partition(arr, low, high)
        
        # Recursively sort left and right subarrays
        quick_sort(arr, low, pivot_index - 1)
        quick_sort(arr, pivot_index + 1, high)
    
    return arr

def partition(arr, low, high):
    # Choose rightmost element as pivot
    pivot = arr[high]
    i = low - 1  # Index of smaller element
    
    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]
    
    # Place pivot in correct position
    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1
```

**Example Trace:**
```
Initial: [10, 80, 30, 90, 40, 50, 70]
Pivot: 70

Partition:
[10, 30, 40, 50, 70, 90, 80]
          pivot at index 4

Recursively sort:
Left:  [10, 30, 40, 50] (elements < 70)
Right: [90, 80] (elements > 70)

Continue recursively:
Left subarray [10, 30, 40, 50]:
  Pivot: 50 → [10, 30, 40, 50]
  
Right subarray [90, 80]:
  Pivot: 80 → [80, 90]

Final: [10, 30, 40, 50, 70, 80, 90]
```

**Pivot Selection Strategies:**
1. **First element**: Simple but can lead to poor performance
2. **Last element**: Most common in implementations
3. **Middle element**: Good average case performance
4. **Random element**: Helps avoid worst-case scenarios
5. **Median-of-three**: Take median of first, middle, and last

**Partitioning Methods:**
1. **Lomuto partition**: Simpler implementation, used above
2. **Hoare partition**: More efficient, fewer swaps
3. **Dutch National Flag**: Handles duplicate elements efficiently

**Key Characteristics:**
- **In-place**: Uses O(log n) extra space for recursion
- **Unstable**: Doesn't preserve relative order of equal elements
- **Cache-friendly**: Good locality of reference
- **Highly optimizable**: Many variants and optimizations exist

**Advantages:**
- Average case O(n log n) performance
- In-place sorting
- Cache-efficient
- Widely used in practice

**Disadvantages:**
- Worst-case O(n²) performance
- Unstable sorting
- Poor performance on already sorted arrays (with poor pivot choice)

---

*Note: These answers cover the sorting algorithms section (Questions 11-20) from DSAQns01.md. Each answer provides comprehensive explanations with examples, implementations, and analysis suitable for technical interviews.*
